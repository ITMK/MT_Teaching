{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Word Embeddings for Neural Machine Translation – Exploring the Google News Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you can further explore word embeddings for NMT based on a much larger model than in the previous notebook concerned with the fundamentals of word embeddings (which we'll call the *base notebook* from now on). Here, we'll explore Google's gigantic News model, which contains a lot more words, a much larger vocabulary and a higher vector dimensionality than the glove-wiki-gigaword-100 model explored in the base notebook. Accordingly, the Google News model captures more and more fine-grained semantic relations between the words in the model. Also, compared to the gigaword model in the base notebook, you can also query the Google News model for multi-word phrases by using an underscore, e.g., *New_York*.  \n",
    "\n",
    "The code cells for exploring the model will remain unchanged, but the documentation will be much less explicit, since you are already familiar with the basic idea of word embeddings and word embedding models. You can always consult the base notebook for reference purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Housekeeping\n",
    "First, we run the same housekeeping steps as we did in the base notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade to the newest version of pip and install or upgrade the gensim library (if necessary)\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll use the pretrained word2vec-google-news-300 model, which is also provided in the [gensim-data](https://github.com/RaRe-Technologies/gensim-data) repository. Since this model was originally trained using word2vec, no prior GloVe → word2vec conversion was necessary in this case. The word2vec-google-news-300 model contains about 100 billion(!) words from the Google News dataset and a vocabulary of about 3 million unique words and phrases. The corresponding vectors have a dimensionality of 300. The model size is about 1.66 GB, so loading it will take quite some time. With an internet connection speed of 500 mbps, loading the model takes about 10 minutes. So, run the code below and grab a coffee or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pretrained word2vec-google-news-300 model from gensim-data and store it in 'word_embeddings'\n",
    "import gensim.downloader as api\n",
    "\n",
    "word_embeddings = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took a while, but now you're ready to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Exploring the Google News model\n",
    "Recall that the embedding matrix of a word embedding model is the product of the vocabulary size and the dimensionality of the embedding vectors. So, the size of our embedding matrix is $3.000.000\\;x\\;300 = 900.000.000$. In other words, the Google News model has an embedding matrix size of 900 million!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of the 150 most frequent words in the model\n",
    "word_embeddings.index_to_key[:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Exploring individual word vectors\n",
    "When we query our  model/embedding matrix as a look-up table, it will return an array of 300 floating point numbers, corresponding to a 300-dimensional vector. In the code cell below, we ask the model to give us the word vector for the multi-word unit *New York*. Remember that the underscore _ is used when you want to query the model for such multi-word units. Again, feel free to modify the code in order to explore other word vectors in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display individual word vectors\n",
    "word_embeddings['New_York']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the base notebook, the standard transformer architecture for NMT works with a vector dimensionality of 512, so the vector representations in these systems will still be bigger than the vector representations in our big Google News model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Exploring the most similar words\n",
    "Exploring the most similar words require some heavy calculation on the part of our model, so don't be surprised if the notebook will be busy for a minute or two (it will be much quicker for subsequent processes). If you query the model for the words most similar to *father*, you'll see that the list now also contains a multi-word unit (*eldest_son*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the model for the words most similar to the word 'father'\n",
    "word_embeddings.most_similar('father')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Exploring the least similar words\n",
    "Again, you can also query the model for words most dissimilar to another word, but the results may not make that much sense to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the parameter 'negative=' reverses the most.similar() function\n",
    "word_embeddings.most_similar(negative=['father'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Calculating the similarity between two specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate semantic similarity between words using the similarity() function\n",
    "word_embeddings.similarity('father', 'nephew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries required to visualise the word embedding vectors\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create the visual representations\n",
    "plt.figure(figsize=(30,3))\n",
    "sns.heatmap([word_embeddings[\"father\"], word_embeddings[\"son\"], word_embeddings[\"government\"]], xticklabels=True, yticklabels=True, cbar=True,\n",
    "            vmin=-1, vmax=1, linewidths=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Identifying semantic outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify semantic outliers using the doesnt_match() function\n",
    "word_embeddings.doesnt_match(['father', 'mother', 'uncle', 'car'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Identifying analogies\n",
    "As in the model used in the base notebook, analogy identification does not work that well even in our bigger Google News model. For example, if we use 'Germany' is to 'Berlin' as 'France' is to '?', the model will not output the correct city. You can have a go and try to come up with examples where the model will identify the correct analogy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify top 3 analogy candidates\n",
    "word_embeddings.most_similar(['Germany', 'Berlin'], ['France'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Visualizing the spatial arrangement of word embeddings using t-SNE\n",
    "Run the code below to import the libraries required for visualising the word embeddings in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries required to perform t-SNE dimensionality reductions and visualizations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, you can manipulate the argument *topn=30* in the fourth line of code from the top in order  to determine the number of words to be plotted. If the function is called without the *topn* argument, the standard value will be 10 (see the base notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which displays our word embeddings in a two-dimensional scatter plot\n",
    "def display_tsne_reduction(model, word):\n",
    "    \n",
    "    arr = np.empty((0,300), dtype='float')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # Get the words most similar to our input word\n",
    "    similar_words = model.similar_by_word(word, topn=30)\n",
    "    \n",
    "    # Add the vector for each of these words to an array\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in similar_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # Calculate the t-SNE coordinates for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    \n",
    "    # Define the visiual representation of our scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to visualize the top *n* words which exhibited the closest proximity to the word passed as second argument to the *display_tsne_reduction()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display word embeddings in two-dimensional space\n",
    "display_tsne_reduction(word_embeddings, 'father')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Tips for further reading\n",
    "The reading tips are the same as in the base notebook. If you read through the tips listed here, you'll acquire a profound understanding of the concepts covered in the two notebooks.  \n",
    "- [Alammar, Jay (2019): The Illustrated Word2vec](http://jalammar.github.io/illustrated-word2vec/)\n",
    "- [Collis, Jaron (2017): Glossary of Deep Learning: Word Embeddings](https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca)  \n",
    "- [Jedamski, Derek (2020): Advanced NLP with Python for Machine Learning](https://www.linkedin.com/learning/advanced-nlp-with-python-for-machine-learning), LinkedIn Learning course (free for students of TH Köln)\n",
    "- [McCormick, Chris (2016): Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)  \n",
    "- [McCormick, Chris (2017): Word2Vec Tutorial Part 2 - Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n",
    "-[official gensim documentation](https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
